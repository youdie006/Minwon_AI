#!/usr/bin/env python3
"""
ìµœì í™”ëœ ì¶”ë¡  íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸
- ë‹¤ì–‘í•œ ìƒì„± ì „ëµ ë¹„êµ
- ìµœì  íŒŒë¼ë¯¸í„° ë„ì¶œ
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from inference_filter import GenerationFilter
import warnings
warnings.filterwarnings("ignore")

class OptimizedGenerator:
    """ìµœì í™”ëœ ìƒì„±ê¸°"""
    
    def __init__(self, model_path: str, model_id: str):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: LoRA ëª¨ë¸ ê²½ë¡œ
            model_id: ë² ì´ìŠ¤ ëª¨ë¸ ID
        """
        self.model_path = model_path
        self.model_id = model_id
        self.filter = GenerationFilter()
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # 4-bit ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            quantization_config=bnb_config,
            device_map="auto",
            max_memory={0: "6GB", "cpu": "10GB"},
            offload_folder="offload",
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        self.model = PeftModel.from_pretrained(base_model, self.model_path)
        self.model.eval()
        
        print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    def generate(self, prompt: str, strategy: dict) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            strategy: ìƒì„± ì „ëµ ì„¤ì •
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                **strategy
            )
        
        # ë””ì½”ë”©
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = response.split("GPT4 Correct Assistant:")[-1].strip()
        
        # í•„í„° ì ìš©
        filtered = self.filter.filter(generated, max_length=200)
        
        return filtered
    
    def test_strategies(self, test_case: dict):
        """
        ë‹¤ì–‘í•œ ì „ëµ í…ŒìŠ¤íŠ¸
        
        Args:
            test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (instruction, input)
        """
        # ë‹¤ì–‘í•œ ìƒì„± ì „ëµ
        strategies = [
            {
                "name": "ì „ëµ 1: ê°•í•œ ë°˜ë³µ íŒ¨ë„í‹°",
                "config": {
                    "max_new_tokens": 150,
                    "temperature": 0.5,
                    "do_sample": True,
                    "top_p": 0.8,
                    "repetition_penalty": 2.0,
                    "no_repeat_ngram_size": 3,
                }
            },
            {
                "name": "ì „ëµ 2: Greedy ë””ì½”ë”©",
                "config": {
                    "max_new_tokens": 150,
                    "do_sample": False,
                    "repetition_penalty": 1.5,
                    "no_repeat_ngram_size": 3,
                }
            },
            {
                "name": "ì „ëµ 3: Contrastive Search",
                "config": {
                    "max_new_tokens": 150,
                    "penalty_alpha": 0.6,
                    "top_k": 4,
                }
            },
            {
                "name": "ì „ëµ 4: ë§¤ìš° ë‚®ì€ temperature",
                "config": {
                    "max_new_tokens": 150,
                    "temperature": 0.3,
                    "do_sample": True,
                    "top_p": 0.7,
                    "top_k": 40,
                    "repetition_penalty": 1.8,
                    "no_repeat_ngram_size": 3,
                }
            },
            {
                "name": "ì „ëµ 5: Beam Search",
                "config": {
                    "max_new_tokens": 150,
                    "num_beams": 3,
                    "no_repeat_ngram_size": 3,
                    "early_stopping": True,
                    "repetition_penalty": 1.3,
                }
            },
            {
                "name": "ì „ëµ 6: ìµœì  ì¡°í•©",
                "config": {
                    "max_new_tokens": 100,
                    "temperature": 0.4,
                    "do_sample": True,
                    "top_p": 0.75,
                    "top_k": 30,
                    "repetition_penalty": 2.5,
                    "no_repeat_ngram_size": 4,
                    "encoder_repetition_penalty": 1.0,
                }
            }
        ]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë“¤
        prompt_templates = [
            # ê¸°ë³¸ í…œí”Œë¦¿
            """GPT4 Correct User: {instruction}

{input}<|end_of_turn|>GPT4 Correct Assistant:""",
            
            # ê°œì„ ëœ í…œí”Œë¦¿
            """GPT4 Correct User: {instruction}

ì…ë ¥: {input}

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ 100ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.<|end_of_turn|>GPT4 Correct Assistant:""",
            
            # Few-shot í…œí”Œë¦¿
            """GPT4 Correct User: ë‹¤ìŒ ë¯¼ì›ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì…ë ¥: ë„ë¡œì— í¬íŠ¸í™€ì´ ìƒê²¨ì„œ ìœ„í—˜í•©ë‹ˆë‹¤.

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.<|end_of_turn|>GPT4 Correct Assistant: ë„ë¡œ í¬íŠ¸í™€ ë°œìƒìœ¼ë¡œ ì¸í•œ ì•ˆì „ ë¬¸ì œ ë¯¼ì›ì…ë‹ˆë‹¤. ì‹ ì†í•œ ë³´ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.<|end_of_turn|>GPT4 Correct User: {instruction}

ì…ë ¥: {input}

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.<|end_of_turn|>GPT4 Correct Assistant:"""
        ]
        
        print("\n=== ì „ëµë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===\n")
        print(f"ğŸ“ ì§€ì‹œ: {test_case['instruction']}")
        print(f"ğŸ“¥ ì…ë ¥: {test_case['input'][:100]}...")
        print("=" * 80)
        
        best_result = None
        best_score = -1
        
        for strategy in strategies:
            print(f"\nğŸ”§ {strategy['name']}")
            print(f"   ì„¤ì •: {strategy['config']}")
            print("-" * 60)
            
            results = []
            for i, template in enumerate(prompt_templates, 1):
                prompt = template.format(
                    instruction=test_case['instruction'],
                    input=test_case['input']
                )
                
                try:
                    generated = self.generate(prompt, strategy['config'])
                    
                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    score = self.calculate_quality_score(generated)
                    results.append((generated, score))
                    
                    if score > best_score:
                        best_score = score
                        best_result = {
                            'strategy': strategy['name'],
                            'template': f'í…œí”Œë¦¿ {i}',
                            'text': generated,
                            'score': score
                        }
                    
                    print(f"   í…œí”Œë¦¿ {i}: {generated[:150]}...")
                    print(f"   ì ìˆ˜: {score:.2f}")
                    
                except Exception as e:
                    print(f"   í…œí”Œë¦¿ {i}: ì˜¤ë¥˜ - {str(e)[:50]}")
                    results.append(("ì˜¤ë¥˜", 0))
            
            # í‰ê·  ì ìˆ˜
            avg_score = sum(r[1] for r in results) / len(results)
            print(f"   í‰ê·  ì ìˆ˜: {avg_score:.2f}")
        
        print("\n" + "=" * 80)
        print("\nğŸ† ìµœì  ê²°ê³¼:")
        if best_result:
            print(f"   ì „ëµ: {best_result['strategy']}")
            print(f"   í…œí”Œë¦¿: {best_result['template']}")
            print(f"   ì ìˆ˜: {best_result['score']:.2f}")
            print(f"   ìƒì„±: {best_result['text']}")
        
        return best_result
    
    def calculate_quality_score(self, text: str) -> float:
        """
        ìƒì„± í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        
        Args:
            text: í‰ê°€í•  í…ìŠ¤íŠ¸
            
        Returns:
            í’ˆì§ˆ ì ìˆ˜ (0-100)
        """
        score = 100.0
        
        # ê¸¸ì´ ì²´í¬
        words = text.split()
        if len(words) < 5:
            score -= 30
        elif len(words) > 150:
            score -= 20
        
        # í•œêµ­ì–´ ë¹„ìœ¨
        if not self.filter.validate_korean(text):
            score -= 50
        
        # ë°˜ë³µ ì²´í¬
        has_repetition, _ = self.filter.detect_word_repetition(text)
        if has_repetition:
            score -= 40
        
        # íŠ¹ìˆ˜ë¬¸ì ê³¼ë‹¤
        special_ratio = len([c for c in text if not c.isalnum() and not c.isspace()]) / max(len(text), 1)
        if special_ratio > 0.3:
            score -= 20
        
        # ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ ì²´í¬
        if "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤." in text:
            score = 10
        
        return max(0, score)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=== ìµœì í™”ëœ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ===\n")
    
    # ì„¤ì •
    model_path = "../ai-service/models/lora_gen_4bit"
    model_id = "openchat/openchat-3.5-0106"
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = OptimizedGenerator(model_path, model_id)
    generator.load_model()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì› ë‚´ìš©ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "input": "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ooë™ì— ê±°ì£¼í•˜ëŠ” ì£¼ë¯¼ì…ë‹ˆë‹¤. ìµœê·¼ ìš°ë¦¬ ë™ë„¤ ê³µì›ì— ì“°ë ˆê¸°ê°€ ë§ì´ ìŒ“ì—¬ ìˆì–´ ë¶ˆí¸ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì£¼ë§ì´ ì§€ë‚˜ë©´ ìŒì‹ë¬¼ ì“°ë ˆê¸°ì™€ ì¼íšŒìš©í’ˆì´ ê³³ê³³ì— ë²„ë ¤ì ¸ ìˆì–´ ì•…ì·¨ê°€ ë‚˜ê³  ë¯¸ê´€ìƒ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ì²­ì—ì„œ ì •ê¸°ì ì¸ ì²­ì†Œì™€ ì“°ë ˆê¸°í†µ ì¶”ê°€ ì„¤ì¹˜ë¥¼ ê²€í† í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            "input": "íš¡ë‹¨ë³´ë„ ì‹ í˜¸ë“± ì‹œê°„ì´ ë„ˆë¬´ ì§§ì•„ì„œ ë…¸ì¸ë¶„ë“¤ì´ ê±´ë„ˆê¸° ì–´ë µìŠµë‹ˆë‹¤. ì‹ í˜¸ ì‹œê°„ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
            "input": "ìš°ë¦¬ ì•„íŒŒíŠ¸ ì• ë„ë¡œì—ì„œ ë°¤ë§ˆë‹¤ ì˜¤í† ë°”ì´ ì†ŒìŒì´ ì‹¬í•©ë‹ˆë‹¤. íŠ¹íˆ ìƒˆë²½ ì‹œê°„ëŒ€ì— êµ‰ìŒì„ ë‚´ë©° ì§€ë‚˜ê°€ëŠ” ì˜¤í† ë°”ì´ ë•Œë¬¸ì— ì ì„ ì„¤ì¹˜ëŠ” ë‚ ì´ ë§ìŠµë‹ˆë‹¤. ë‹¨ì†ì„ ê°•í™”í•´ ì£¼ì‹œê³ , ë°©ì§€í„±ì´ë‚˜ CCTV ì„¤ì¹˜ë„ ê²€í† í•´ ì£¼ì„¸ìš”."
        }
    ]
    
    # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    best_configs = []
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n\n{'#' * 80}")
        print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}")
        print('#' * 80)
        
        best = generator.test_strategies(test_case)
        if best:
            best_configs.append(best)
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n\n" + "=" * 80)
    print("ğŸ“Š ìµœì¢… ìš”ì•½")
    print("=" * 80)
    
    if best_configs:
        avg_score = sum(c['score'] for c in best_configs) / len(best_configs)
        print(f"\ní‰ê·  ì ìˆ˜: {avg_score:.2f}")
        
        # ê°€ì¥ ë§ì´ ì„ íƒëœ ì „ëµ
        strategies = [c['strategy'] for c in best_configs]
        most_common = max(set(strategies), key=strategies.count)
        print(f"ìµœì  ì „ëµ: {most_common}")
        
        # ê°€ì¥ ë§ì´ ì„ íƒëœ í…œí”Œë¦¿
        templates = [c['template'] for c in best_configs]
        most_common_template = max(set(templates), key=templates.count)
        print(f"ìµœì  í…œí”Œë¦¿: {most_common_template}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ ìµœëŒ€ VRAM ì‚¬ìš©: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")


if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\n")
        torch.cuda.empty_cache()
    
    main()