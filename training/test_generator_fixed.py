#!/usr/bin/env python3
"""
ìƒì„± ëª¨ë¸ ë°˜ë³µ ë¬¸ì œ í•´ê²° í…ŒìŠ¤íŠ¸
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import json
import warnings
import re
warnings.filterwarnings("ignore")

def detect_repetition(text, min_length=10, max_repeats=3):
    """í…ìŠ¤íŠ¸ì—ì„œ ë°˜ë³µ íŒ¨í„´ ê°ì§€"""
    words = text.split()
    for length in range(min_length, len(words) // max_repeats + 1):
        for i in range(len(words) - length * max_repeats + 1):
            pattern = words[i:i+length]
            is_repeated = True
            for j in range(1, max_repeats):
                if words[i+j*length:i+(j+1)*length] != pattern:
                    is_repeated = False
                    break
            if is_repeated:
                return True, i + length
    return False, len(words)

def test_generator_with_fixes():
    """ìƒì„± ëª¨ë¸ í…ŒìŠ¤íŠ¸ - ë°˜ë³µ ë¬¸ì œ í•´ê²° í¬í•¨"""
    print("=== ìƒì„± ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ë°˜ë³µ ë¬¸ì œ í•´ê²°) ===\n")
    
    model_path = "../ai-service/models/lora_gen_4bit"
    model_id = "openchat/openchat-3.5-0106"
    
    # 4-bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory={0: "6GB", "cpu": "10GB"},
        offload_folder="offload",
    )
    
    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì› ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "input": "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ooë™ì— ê±°ì£¼í•˜ëŠ” ì£¼ë¯¼ì…ë‹ˆë‹¤. ìµœê·¼ ìš°ë¦¬ ë™ë„¤ ê³µì›ì— ì“°ë ˆê¸°ê°€ ë§ì´ ìŒ“ì—¬ ìˆì–´ ë¶ˆí¸ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì£¼ë§ì´ ì§€ë‚˜ë©´ ìŒì‹ë¬¼ ì“°ë ˆê¸°ì™€ ì¼íšŒìš©í’ˆì´ ê³³ê³³ì— ë²„ë ¤ì ¸ ìˆì–´ ì•…ì·¨ê°€ ë‚˜ê³  ë¯¸ê´€ìƒ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ì²­ì—ì„œ ì •ê¸°ì ì¸ ì²­ì†Œì™€ ì“°ë ˆê¸°í†µ ì¶”ê°€ ì„¤ì¹˜ë¥¼ ê²€í† í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            "input": "íš¡ë‹¨ë³´ë„ ì‹ í˜¸ë“± ì‹œê°„ì´ ë„ˆë¬´ ì§§ì•„ì„œ ë…¸ì¸ë¶„ë“¤ì´ ê±´ë„ˆê¸° ì–´ë µìŠµë‹ˆë‹¤. ì‹ í˜¸ ì‹œê°„ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
            "input": "ìš°ë¦¬ ì•„íŒŒíŠ¸ ì• ë„ë¡œì—ì„œ ë°¤ë§ˆë‹¤ ì˜¤í† ë°”ì´ ì†ŒìŒì´ ì‹¬í•©ë‹ˆë‹¤. íŠ¹íˆ ìƒˆë²½ ì‹œê°„ëŒ€ì— êµ‰ìŒì„ ë‚´ë©° ì§€ë‚˜ê°€ëŠ” ì˜¤í† ë°”ì´ ë•Œë¬¸ì— ì ì„ ì„¤ì¹˜ëŠ” ë‚ ì´ ë§ìŠµë‹ˆë‹¤. ë‹¨ì†ì„ ê°•í™”í•´ ì£¼ì‹œê³ , ë°©ì§€í„±ì´ë‚˜ CCTV ì„¤ì¹˜ë„ ê²€í† í•´ ì£¼ì„¸ìš”."
        }
    ]
    
    # ë‹¤ì–‘í•œ ìƒì„± ì„¤ì • í…ŒìŠ¤íŠ¸
    generation_configs = [
        {
            "name": "ê¸°ë³¸ ì„¤ì • (repetition_penalty 1.5)",
            "config": {
                "max_new_tokens": 300,
                "temperature": 0.7,
                "do_sample": True,
                "top_p": 0.9,
                "repetition_penalty": 1.5,  # ì¦ê°€
                "no_repeat_ngram_size": 3,  # 3-gram ë°˜ë³µ ë°©ì§€
            }
        },
        {
            "name": "ê°•í™”ëœ ì„¤ì • (repetition_penalty 2.0)",
            "config": {
                "max_new_tokens": 300,
                "temperature": 0.8,
                "do_sample": True,
                "top_p": 0.95,
                "top_k": 50,  # top-k ìƒ˜í”Œë§ ì¶”ê°€
                "repetition_penalty": 2.0,  # ë” ì¦ê°€
                "no_repeat_ngram_size": 4,  # 4-gram ë°˜ë³µ ë°©ì§€
            }
        },
        {
            "name": "Beam Search (deterministic)",
            "config": {
                "max_new_tokens": 300,
                "num_beams": 4,
                "no_repeat_ngram_size": 3,
                "early_stopping": True,
                "repetition_penalty": 1.2,
            }
        },
        {
            "name": "ë‚®ì€ temperature + ë†’ì€ penalty",
            "config": {
                "max_new_tokens": 300,
                "temperature": 0.5,  # ë‚®ì€ temperature
                "do_sample": True,
                "top_p": 0.8,
                "repetition_penalty": 1.8,
                "no_repeat_ngram_size": 3,
                "encoder_repetition_penalty": 1.0,  # ì…ë ¥ í…ìŠ¤íŠ¸ ë°˜ë³µ ë°©ì§€
            }
        }
    ]
    
    print("\n=== ìƒì„± ê²°ê³¼ ë¹„êµ ===\n")
    
    # ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ë‹¤ì–‘í•œ ì„¤ì • í…ŒìŠ¤íŠ¸
    test = test_cases[0]
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤:")
    print(f"ğŸ“ ì§€ì‹œ: {test['instruction']}")
    print(f"ğŸ“¥ ì…ë ¥: {test['input'][:100]}...")
    print("\n" + "="*80 + "\n")
    
    for config_info in generation_configs:
        print(f"\nğŸ”§ {config_info['name']}")
        print(f"   ì„¤ì •: {config_info['config']}")
        print("-" * 60)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""GPT4 Correct User: {test['instruction']}

{test['input']}<|end_of_turn|>GPT4 Correct Assistant:"""
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                **config_info['config']
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = response.split("GPT4 Correct Assistant:")[-1].strip()
        
        # ë°˜ë³µ ê°ì§€
        has_repetition, stop_idx = detect_repetition(generated)
        
        if has_repetition:
            print(f"âš ï¸  ë°˜ë³µ ê°ì§€ë¨! (ìœ„ì¹˜: {stop_idx} ë‹¨ì–´)")
            # ë°˜ë³µ ì‹œì‘ ì „ê¹Œì§€ë§Œ ì¶œë ¥
            generated_words = generated.split()[:stop_idx]
            generated_clean = " ".join(generated_words)
            print(f"ğŸ“¤ ìƒì„± ë‹µë³€ (ë°˜ë³µ ì œê±°): {generated_clean}")
        else:
            print(f"âœ… ë°˜ë³µ ì—†ìŒ")
            print(f"ğŸ“¤ ìƒì„± ë‹µë³€: {generated}")
        
        print(f"   í† í° ìˆ˜: {len(tokenizer.encode(generated))}")
        print("-" * 60)
    
    print("\n" + "="*80 + "\n")
    print("\n=== ìµœì  ì„¤ì •ìœ¼ë¡œ ì „ì²´ í…ŒìŠ¤íŠ¸ ===\n")
    
    # ê°€ì¥ ì¢‹ì€ ì„¤ì •ìœ¼ë¡œ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    best_config = {
        "max_new_tokens": 300,
        "temperature": 0.7,
        "do_sample": True,
        "top_p": 0.9,
        "top_k": 50,
        "repetition_penalty": 1.8,
        "no_repeat_ngram_size": 3,
        "encoder_repetition_penalty": 1.0,
    }
    
    print(f"ì„ íƒëœ ìµœì  ì„¤ì •: {best_config}\n")
    
    for i, test in enumerate(test_cases, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}:")
        print(f"ğŸ“ ì§€ì‹œ: {test['instruction']}")
        print(f"ğŸ“¥ ì…ë ¥: {test['input'][:100]}...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""GPT4 Correct User: {test['instruction']}

{test['input']}<|end_of_turn|>GPT4 Correct Assistant:"""
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                **best_config
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = response.split("GPT4 Correct Assistant:")[-1].strip()
        
        # ë°˜ë³µ ê°ì§€ ë° ì²˜ë¦¬
        has_repetition, stop_idx = detect_repetition(generated)
        
        if has_repetition:
            generated_words = generated.split()[:stop_idx]
            generated = " ".join(generated_words)
            print(f"âš ï¸  ë°˜ë³µ ì œê±° ì ìš©")
        
        print(f"ğŸ“¤ ìƒì„± ë‹µë³€: {generated}")
        print("-" * 60)
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ ìµœëŒ€ VRAM ì‚¬ìš©: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\n")
        torch.cuda.empty_cache()
    
    test_generator_with_fixes()