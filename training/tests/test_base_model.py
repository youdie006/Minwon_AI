#!/usr/bin/env python3
"""
ë² ì´ìŠ¤ ëª¨ë¸ê³¼ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")

def test_models():
    """ë² ì´ìŠ¤ ëª¨ë¸ê³¼ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¹„êµ"""
    print("=== ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ===\n")
    
    model_id = "openchat/openchat-3.5-0106"
    lora_path = "../ai-service/models/lora_gen_4bit"
    
    # 4-bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_case = {
        "instruction": "ë‹¤ìŒ ë¯¼ì› ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
        "input": "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ooë™ì— ê±°ì£¼í•˜ëŠ” ì£¼ë¯¼ì…ë‹ˆë‹¤. ìµœê·¼ ìš°ë¦¬ ë™ë„¤ ê³µì›ì— ì“°ë ˆê¸°ê°€ ë§ì´ ìŒ“ì—¬ ìˆì–´ ë¶ˆí¸ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì£¼ë§ì´ ì§€ë‚˜ë©´ ìŒì‹ë¬¼ ì“°ë ˆê¸°ì™€ ì¼íšŒìš©í’ˆì´ ê³³ê³³ì— ë²„ë ¤ì ¸ ìˆì–´ ì•…ì·¨ê°€ ë‚˜ê³  ë¯¸ê´€ìƒ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ì²­ì—ì„œ ì •ê¸°ì ì¸ ì²­ì†Œì™€ ì“°ë ˆê¸°í†µ ì¶”ê°€ ì„¤ì¹˜ë¥¼ ê²€í† í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
    }
    
    # ìƒì„± ì„¤ì •
    gen_config = {
        "max_new_tokens": 150,
        "temperature": 0.7,
        "do_sample": True,
        "top_p": 0.9,
        "repetition_penalty": 1.2,
        "no_repeat_ngram_size": 3,
    }
    
    print("1. ë² ì´ìŠ¤ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("-" * 60)
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    print("ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì¤‘...")
    base_tokenizer = AutoTokenizer.from_pretrained(model_id)
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory={0: "6GB", "cpu": "10GB"},
        offload_folder="offload",
    )
    base_model.eval()
    
    # ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ìƒì„±
    prompt = f"""GPT4 Correct User: {test_case['instruction']}

{test_case['input']}<|end_of_turn|>GPT4 Correct Assistant:"""
    
    inputs = base_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.cuda() for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = base_model.generate(
            **inputs,
            pad_token_id=base_tokenizer.eos_token_id,
            eos_token_id=base_tokenizer.eos_token_id,
            **gen_config
        )
    
    response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated = response.split("GPT4 Correct Assistant:")[-1].strip()
    
    print(f"ğŸ“ ì§€ì‹œ: {test_case['instruction']}")
    print(f"ğŸ“¥ ì…ë ¥: {test_case['input'][:100]}...")
    print(f"ğŸ“¤ ë² ì´ìŠ¤ ëª¨ë¸ ë‹µë³€:\n{generated}\n")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del base_model
    torch.cuda.empty_cache()
    
    print("\n2. íŒŒì¸íŠœë‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("-" * 60)
    
    # íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ
    print("íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë”© ì¤‘...")
    ft_tokenizer = AutoTokenizer.from_pretrained(lora_path)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory={0: "6GB", "cpu": "10GB"},
        offload_folder="offload",
    )
    
    ft_model = PeftModel.from_pretrained(base_model, lora_path)
    ft_model.eval()
    
    # íŒŒì¸íŠœë‹ ëª¨ë¸ë¡œ ìƒì„±
    inputs = ft_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.cuda() for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = ft_model.generate(
            **inputs,
            pad_token_id=ft_tokenizer.eos_token_id,
            eos_token_id=ft_tokenizer.eos_token_id,
            **gen_config
        )
    
    response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated = response.split("GPT4 Correct Assistant:")[-1].strip()
    
    print(f"ğŸ“ ì§€ì‹œ: {test_case['instruction']}")
    print(f"ğŸ“¥ ì…ë ¥: {test_case['input'][:100]}...")
    print(f"ğŸ“¤ íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€:\n{generated}\n")
    
    print("\n3. ì¶”ê°€ í…ŒìŠ¤íŠ¸ - ê°„ë‹¨í•œ ì§ˆë¬¸")
    print("-" * 60)
    
    simple_tests = [
        "ì•ˆë…•í•˜ì„¸ìš”?",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?",
        "ë¯¼ì› ì²˜ë¦¬ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
    ]
    
    for question in simple_tests:
        prompt = f"GPT4 Correct User: {question}<|end_of_turn|>GPT4 Correct Assistant:"
        
        inputs = ft_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = ft_model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.2,
                pad_token_id=ft_tokenizer.eos_token_id,
                eos_token_id=ft_tokenizer.eos_token_id,
            )
        
        response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = response.split("GPT4 Correct Assistant:")[-1].strip()
        
        print(f"Q: {question}")
        print(f"A: {generated}\n")
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ ìµœëŒ€ VRAM ì‚¬ìš©: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\n")
        torch.cuda.empty_cache()
    
    test_models()