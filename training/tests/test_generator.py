#!/usr/bin/env python3
"""
ìƒì„± ëª¨ë¸ ê°„ë‹¨ í…ŒìŠ¤íŠ¸
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import json
import warnings
warnings.filterwarnings("ignore")

def test_generator():
    """ìƒì„± ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("=== ìƒì„± ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===\n")
    
    model_path = "../ai-service/models/lora_gen_4bit"
    model_id = "openchat/openchat-3.5-0106"
    
    # 4-bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory={0: "6GB", "cpu": "10GB"},
        offload_folder="offload",
    )
    
    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì› ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "input": "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ooë™ì— ê±°ì£¼í•˜ëŠ” ì£¼ë¯¼ì…ë‹ˆë‹¤. ìµœê·¼ ìš°ë¦¬ ë™ë„¤ ê³µì›ì— ì“°ë ˆê¸°ê°€ ë§ì´ ìŒ“ì—¬ ìˆì–´ ë¶ˆí¸ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì£¼ë§ì´ ì§€ë‚˜ë©´ ìŒì‹ë¬¼ ì“°ë ˆê¸°ì™€ ì¼íšŒìš©í’ˆì´ ê³³ê³³ì— ë²„ë ¤ì ¸ ìˆì–´ ì•…ì·¨ê°€ ë‚˜ê³  ë¯¸ê´€ìƒ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ì²­ì—ì„œ ì •ê¸°ì ì¸ ì²­ì†Œì™€ ì“°ë ˆê¸°í†µ ì¶”ê°€ ì„¤ì¹˜ë¥¼ ê²€í† í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            "input": "íš¡ë‹¨ë³´ë„ ì‹ í˜¸ë“± ì‹œê°„ì´ ë„ˆë¬´ ì§§ì•„ì„œ ë…¸ì¸ë¶„ë“¤ì´ ê±´ë„ˆê¸° ì–´ë µìŠµë‹ˆë‹¤. ì‹ í˜¸ ì‹œê°„ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
            "input": "ìš°ë¦¬ ì•„íŒŒíŠ¸ ì• ë„ë¡œì—ì„œ ë°¤ë§ˆë‹¤ ì˜¤í† ë°”ì´ ì†ŒìŒì´ ì‹¬í•©ë‹ˆë‹¤. íŠ¹íˆ ìƒˆë²½ ì‹œê°„ëŒ€ì— êµ‰ìŒì„ ë‚´ë©° ì§€ë‚˜ê°€ëŠ” ì˜¤í† ë°”ì´ ë•Œë¬¸ì— ì ì„ ì„¤ì¹˜ëŠ” ë‚ ì´ ë§ìŠµë‹ˆë‹¤. ë‹¨ì†ì„ ê°•í™”í•´ ì£¼ì‹œê³ , ë°©ì§€í„±ì´ë‚˜ CCTV ì„¤ì¹˜ë„ ê²€í† í•´ ì£¼ì„¸ìš”."
        }
    ]
    
    print("\n=== ìƒì„± ê²°ê³¼ ===\n")
    
    for i, test in enumerate(test_cases, 1):
        print(f"í…ŒìŠ¤íŠ¸ {i}:")
        print(f"ğŸ“ ì§€ì‹œ: {test['instruction']}")
        print(f"ğŸ“¥ ì…ë ¥: {test['input'][:100]}...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""GPT4 Correct User: {test['instruction']}

{test['input']}<|end_of_turn|>GPT4 Correct Assistant:"""
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=1000,  # ì¶©ë¶„íˆ ê¸´ ë‹µë³€ì„ ìœ„í•´ 1000í† í°ìœ¼ë¡œ ì„¤ì •
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
                min_length=100  # ìµœì†Œ ê¸¸ì´ ë³´ì¥
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = response.split("GPT4 Correct Assistant:")[-1].strip()
        
        print(f"ğŸ“¤ ìƒì„± ë‹µë³€: {generated}")
        print("-" * 60)
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ ìµœëŒ€ VRAM ì‚¬ìš©: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\n")
        torch.cuda.empty_cache()
    
    test_generator()