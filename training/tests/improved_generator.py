#!/usr/bin/env python3
"""
ê°œì„ ëœ ìƒì„± ëª¨ë¸ - ëª¨ë“  ìµœì í™” í†µí•©
- ë°˜ë³µ ì œê±° í•„í„°
- ìµœì í™”ëœ ìƒì„± íŒŒë¼ë¯¸í„°
- ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import sys
sys.path.append("..")
from utils.inference_filter import GenerationFilter
import warnings
warnings.filterwarnings("ignore")

class ImprovedMinwonGenerator:
    """ê°œì„ ëœ ë¯¼ì› ìƒì„± ëª¨ë¸"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.model_path = "../ai-service/models/lora_gen_4bit"
        self.model_id = "openchat/openchat-3.5-0106"
        self.filter = GenerationFilter()
        self.model = None
        self.tokenizer = None
        
        # ìµœì  ìƒì„± ì„¤ì • (í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜)
        self.generation_config = {
            "max_new_tokens": 100,
            "temperature": 0.4,
            "do_sample": True,
            "top_p": 0.75,
            "top_k": 30,
            "repetition_penalty": 2.5,
            "no_repeat_ngram_size": 4,
            "encoder_repetition_penalty": 1.0,
        }
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ê°œì„ ëœ ìƒì„± ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # 4-bit ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            quantization_config=bnb_config,
            device_map="auto",
            max_memory={0: "6GB", "cpu": "10GB"},
            offload_folder="offload",
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        self.model = PeftModel.from_pretrained(base_model, self.model_path)
        self.model.eval()
        
        print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    def create_prompt(self, instruction: str, input_text: str, use_few_shot: bool = False) -> str:
        """
        í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            instruction: ì§€ì‹œì‚¬í•­
            input_text: ì…ë ¥ í…ìŠ¤íŠ¸
            use_few_shot: Few-shot ì˜ˆì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ìƒì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        if use_few_shot:
            # Few-shot í”„ë¡¬í”„íŠ¸
            prompt = f"""GPT4 Correct User: ë‹¤ìŒ ë¯¼ì›ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì…ë ¥: ë„ë¡œì— í¬íŠ¸í™€ì´ ìƒê²¨ì„œ ìœ„í—˜í•©ë‹ˆë‹¤. ë¹ ë¥¸ ì¡°ì¹˜ ë°”ëë‹ˆë‹¤.

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.<|end_of_turn|>GPT4 Correct Assistant: ë„ë¡œ í¬íŠ¸í™€ë¡œ ì¸í•œ ì•ˆì „ ë¬¸ì œ ë¯¼ì›ì…ë‹ˆë‹¤. ì‹ ì†í•œ ë³´ìˆ˜ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.<|end_of_turn|>GPT4 Correct User: {instruction}

ì…ë ¥: {input_text}

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. 100ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.<|end_of_turn|>GPT4 Correct Assistant:"""
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""GPT4 Correct User: {instruction}

ì…ë ¥: {input_text}

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. 100ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.<|end_of_turn|>GPT4 Correct Assistant:"""
        
        return prompt
    
    def generate(self, instruction: str, input_text: str, use_filter: bool = True) -> dict:
        """
        í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            instruction: ì§€ì‹œì‚¬í•­
            input_text: ì…ë ¥ í…ìŠ¤íŠ¸
            use_filter: í•„í„° ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ìƒì„± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.model:
            self.load_model()
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.create_prompt(instruction, input_text, use_few_shot=True)
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                **self.generation_config
            )
        
        # ë””ì½”ë”©
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        raw_output = response.split("GPT4 Correct Assistant:")[-1].strip()
        
        # í•„í„° ì ìš©
        if use_filter:
            filtered_output = self.filter.filter(raw_output, max_length=150)
        else:
            filtered_output = raw_output
        
        # í’ˆì§ˆ ê²€ì¦
        is_valid = self.filter.validate_korean(filtered_output)
        
        return {
            "instruction": instruction,
            "input": input_text[:100] + "..." if len(input_text) > 100 else input_text,
            "raw_output": raw_output[:200] + "..." if len(raw_output) > 200 else raw_output,
            "filtered_output": filtered_output,
            "is_valid": is_valid,
            "token_count": len(self.tokenizer.encode(filtered_output))
        }
    
    def batch_generate(self, test_cases: list) -> list:
        """
        ë°°ì¹˜ ìƒì„±
        
        Args:
            test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìƒì„± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, case in enumerate(test_cases, 1):
            print(f"\ní…ŒìŠ¤íŠ¸ {i}/{len(test_cases)}")
            result = self.generate(case["instruction"], case["input"])
            results.append(result)
            
            print(f"ğŸ“ ì§€ì‹œ: {result['instruction']}")
            print(f"ğŸ“¥ ì…ë ¥: {result['input']}")
            print(f"ğŸ“¤ ìƒì„±: {result['filtered_output']}")
            print(f"âœ… ìœ íš¨: {result['is_valid']}")
            print(f"ğŸ“Š í† í°: {result['token_count']}")
        
        return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=== ê°œì„ ëœ ìƒì„± ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===\n")
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = ImprovedMinwonGenerator()
    generator.load_model()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì› ë‚´ìš©ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "input": "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ooë™ì— ê±°ì£¼í•˜ëŠ” ì£¼ë¯¼ì…ë‹ˆë‹¤. ìµœê·¼ ìš°ë¦¬ ë™ë„¤ ê³µì›ì— ì“°ë ˆê¸°ê°€ ë§ì´ ìŒ“ì—¬ ìˆì–´ ë¶ˆí¸ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì£¼ë§ì´ ì§€ë‚˜ë©´ ìŒì‹ë¬¼ ì“°ë ˆê¸°ì™€ ì¼íšŒìš©í’ˆì´ ê³³ê³³ì— ë²„ë ¤ì ¸ ìˆì–´ ì•…ì·¨ê°€ ë‚˜ê³  ë¯¸ê´€ìƒ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ì²­ì—ì„œ ì •ê¸°ì ì¸ ì²­ì†Œì™€ ì“°ë ˆê¸°í†µ ì¶”ê°€ ì„¤ì¹˜ë¥¼ ê²€í† í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            "input": "íš¡ë‹¨ë³´ë„ ì‹ í˜¸ë“± ì‹œê°„ì´ ë„ˆë¬´ ì§§ì•„ì„œ ë…¸ì¸ë¶„ë“¤ì´ ê±´ë„ˆê¸° ì–´ë µìŠµë‹ˆë‹¤. ì‹ í˜¸ ì‹œê°„ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
            "input": "ìš°ë¦¬ ì•„íŒŒíŠ¸ ì• ë„ë¡œì—ì„œ ë°¤ë§ˆë‹¤ ì˜¤í† ë°”ì´ ì†ŒìŒì´ ì‹¬í•©ë‹ˆë‹¤. íŠ¹íˆ ìƒˆë²½ ì‹œê°„ëŒ€ì— êµ‰ìŒì„ ë‚´ë©° ì§€ë‚˜ê°€ëŠ” ì˜¤í† ë°”ì´ ë•Œë¬¸ì— ì ì„ ì„¤ì¹˜ëŠ” ë‚ ì´ ë§ìŠµë‹ˆë‹¤. ë‹¨ì†ì„ ê°•í™”í•´ ì£¼ì‹œê³ , ë°©ì§€í„±ì´ë‚˜ CCTV ì„¤ì¹˜ë„ ê²€í† í•´ ì£¼ì„¸ìš”."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
            "input": "ì£¼ì°¨ì¥ì´ ë¶€ì¡±í•´ì„œ ë¶ˆë²•ì£¼ì°¨ê°€ ë§ìŠµë‹ˆë‹¤. ì£¼ì°¨ê³µê°„ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”."
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì— ëŒ€í•œ ì²˜ë¦¬ ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
            "input": "ê³µì›ì— ê°€ë¡œë“±ì´ ì—†ì–´ì„œ ë°¤ì— ìœ„í—˜í•©ë‹ˆë‹¤."
        }
    ]
    
    # ë°°ì¹˜ ìƒì„±
    print("\nğŸ“‹ ë°°ì¹˜ ìƒì„± ì‹œì‘")
    print("=" * 60)
    results = generator.batch_generate(test_cases)
    
    # ê²°ê³¼ ìš”ì•½
    print("\n\n" + "=" * 60)
    print("ğŸ“Š ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    valid_count = sum(1 for r in results if r['is_valid'])
    avg_tokens = sum(r['token_count'] for r in results) / len(results)
    
    print(f"\nì´ í…ŒìŠ¤íŠ¸: {len(results)}ê°œ")
    print(f"ìœ íš¨ ìƒì„±: {valid_count}ê°œ ({valid_count/len(results)*100:.1f}%)")
    print(f"í‰ê·  í† í°: {avg_tokens:.1f}ê°œ")
    
    # ë¬¸ì œ ì¼€ì´ìŠ¤ ë¶„ì„
    print("\nâš ï¸ ë¬¸ì œ ì¼€ì´ìŠ¤:")
    for i, result in enumerate(results, 1):
        if not result['is_valid']:
            print(f"  - í…ŒìŠ¤íŠ¸ {i}: í•œêµ­ì–´ ë¹„ìœ¨ ë¶€ì¡±")
            print(f"    ì›ë³¸: {result['raw_output'][:100]}...")
    
    # ì„±ê³µ ì˜ˆì‹œ
    print("\nâœ… ì„±ê³µ ì˜ˆì‹œ:")
    for i, result in enumerate(results[:3], 1):
        if result['is_valid']:
            print(f"  {i}. {result['filtered_output']}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ ìµœëŒ€ VRAM ì‚¬ìš©: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")


if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\n")
        torch.cuda.empty_cache()
    
    main()