#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ë² ì´ìŠ¤ ëª¨ë¸ í…ŒìŠ¤íŠ¸
íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ì‚¬ìš©
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import sys
sys.path.append("..")
from utils.inference_filter import GenerationFilter
import warnings
warnings.filterwarnings("ignore")

def test_base_model_only():
    """ë² ì´ìŠ¤ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸"""
    print("=== ë² ì´ìŠ¤ ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ ===\n")
    
    model_id = "openchat/openchat-3.5-0106"
    filter_obj = GenerationFilter()
    
    # 4-bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    print("ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory={0: "6GB", "cpu": "10GB"},
        offload_folder="offload",
    )
    model.eval()
    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "instruction": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
            "input": "The park is dirty with trash everywhere. Need regular cleaning and more trash cans."
        },
        {
            "instruction": "Summarize in Korean:",
            "input": "Citizens complain about noise from motorcycles at night. They request stronger enforcement."
        },
        {
            "instruction": "í•œêµ­ì–´ë¡œ ë‹µë³€:",
            "input": "ê³µì›ì— ì“°ë ˆê¸°ê°€ ë§ìŠµë‹ˆë‹¤. ì²­ì†Œë¥¼ ìš”ì²­í•©ë‹ˆë‹¤."
        }
    ]
    
    # ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ í…ŒìŠ¤íŠ¸
    prompt_formats = [
        # OpenChat ê¸°ë³¸ í˜•ì‹
        lambda inst, inp: f"GPT4 Correct User: {inst}\n\n{inp}<|end_of_turn|>GPT4 Correct Assistant:",
        
        # ë‹¨ìˆœ í˜•ì‹
        lambda inst, inp: f"Instruction: {inst}\nInput: {inp}\nResponse:",
        
        # í•œêµ­ì–´ ê°•ì¡°
        lambda inst, inp: f"User: {inst}\n{inp}\nAssistant (Korean):",
    ]
    
    print("í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    print("=" * 80)
    
    for i, test in enumerate(test_cases, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}:")
        print(f"ì§€ì‹œ: {test['instruction']}")
        print(f"ì…ë ¥: {test['input']}")
        print("-" * 60)
        
        for j, format_fn in enumerate(prompt_formats, 1):
            prompt = format_fn(test['instruction'], test['input'])
            
            # í† í¬ë‚˜ì´ì§•
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # ìƒì„±
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    repetition_penalty=1.5,
                    no_repeat_ngram_size=3,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            # ë””ì½”ë”©
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            if "Assistant:" in response:
                generated = response.split("Assistant:")[-1].strip()
            elif "Response:" in response:
                generated = response.split("Response:")[-1].strip()
            else:
                generated = response[len(prompt):].strip()
            
            # í•„í„° ì ìš©
            filtered = filter_obj.filter(generated, max_length=100)
            
            print(f"\ní˜•ì‹ {j} ê²°ê³¼:")
            print(f"ì›ë³¸: {generated[:150]}...")
            print(f"í•„í„°: {filtered}")
            is_valid = filter_obj.validate_korean(filtered)
            print(f"í•œêµ­ì–´ ìœ íš¨: {is_valid}")
    
    print("\n" + "=" * 80)
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ ìµœëŒ€ VRAM ì‚¬ìš©: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\n")
        torch.cuda.empty_cache()
    
    test_base_model_only()