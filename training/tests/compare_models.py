#!/usr/bin/env python3
"""
ë² ì´ìŠ¤ ëª¨ë¸ê³¼ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¹„êµ ë¶„ì„
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import sys
sys.path.append("..")
from utils.inference_filter import GenerationFilter
import warnings
warnings.filterwarnings("ignore")

def compare_models():
    """ë² ì´ìŠ¤ ëª¨ë¸ê³¼ íŒŒì¸íŠœë‹ ëª¨ë¸ ìƒì„¸ ë¹„êµ"""
    print("=== ë² ì´ìŠ¤ ëª¨ë¸ vs íŒŒì¸íŠœë‹ ëª¨ë¸ ë¹„êµ ===\n")
    
    model_id = "openchat/openchat-3.5-0106"
    lora_path = "../ai-service/models/lora_gen_4bit"
    filter_obj = GenerationFilter()
    
    # 4-bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # ë™ì¼í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "input": "ê³µì›ì— ì“°ë ˆê¸°ê°€ ë§ì´ ìŒ“ì—¬ ìˆìŠµë‹ˆë‹¤. ì •ê¸°ì ì¸ ì²­ì†Œì™€ ì“°ë ˆê¸°í†µ ì¶”ê°€ ì„¤ì¹˜ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.",
            "expected": "ê³µì› ì²­ì†Œ ë° ì“°ë ˆê¸°í†µ ì„¤ì¹˜ ìš”ì²­"
        },
        {
            "instruction": "ë‹¤ìŒ ë¯¼ì›ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            "input": "íš¡ë‹¨ë³´ë„ ì‹ í˜¸ ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.",
            "expected": "ì‹ í˜¸ ì‹œê°„ ì—°ì¥ ê²€í† í•˜ê² ìŠµë‹ˆë‹¤"
        },
        {
            "instruction": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì„¸ìš”.",
            "input": "ì£¼ë¯¼ë“¤ì´ ì•¼ê°„ ì†ŒìŒìœ¼ë¡œ ë¶ˆí¸ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ë‹¨ì†ì„ ê°•í™”í•´ì£¼ì„¸ìš”.",
            "expected": "ì•¼ê°„ ì†ŒìŒ ë‹¨ì† ìš”ì²­"
        }
    ]
    
    # ìƒì„± ì„¤ì • (ë™ì¼í•˜ê²Œ ì ìš©)
    gen_config = {
        "max_new_tokens": 100,
        "temperature": 0.7,
        "do_sample": True,
        "top_p": 0.9,
        "repetition_penalty": 1.5,
        "no_repeat_ngram_size": 3,
    }
    
    results = {"base": [], "finetuned": []}
    
    print("1. ë² ì´ìŠ¤ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("-" * 60)
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    print("ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©...")
    base_tokenizer = AutoTokenizer.from_pretrained(model_id)
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory={0: "6GB", "cpu": "10GB"},
        offload_folder="offload",
    )
    base_model.eval()
    
    # ë² ì´ìŠ¤ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    for i, test in enumerate(test_cases, 1):
        prompt = f"""GPT4 Correct User: {test['instruction']}

{test['input']}<|end_of_turn|>GPT4 Correct Assistant:"""
        
        inputs = base_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = base_model.generate(
                **inputs,
                pad_token_id=base_tokenizer.eos_token_id,
                eos_token_id=base_tokenizer.eos_token_id,
                **gen_config
            )
        
        response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = response.split("GPT4 Correct Assistant:")[-1].strip()
        filtered = filter_obj.filter(generated, max_length=100)
        
        print(f"\ní…ŒìŠ¤íŠ¸ {i}:")
        print(f"ì…ë ¥: {test['input']}")
        print(f"ìƒì„±: {filtered}")
        print(f"ê¸°ëŒ€: {test['expected']}")
        
        results["base"].append({
            "raw": generated,
            "filtered": filtered,
            "valid": filter_obj.validate_korean(filtered)
        })
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del base_model
    torch.cuda.empty_cache()
    
    print("\n\n2. íŒŒì¸íŠœë‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("-" * 60)
    
    # íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ
    print("íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë”©...")
    ft_tokenizer = AutoTokenizer.from_pretrained(lora_path)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory={0: "6GB", "cpu": "10GB"},
        offload_folder="offload",
    )
    
    ft_model = PeftModel.from_pretrained(base_model, lora_path)
    ft_model.eval()
    
    # íŒŒì¸íŠœë‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    for i, test in enumerate(test_cases, 1):
        prompt = f"""GPT4 Correct User: {test['instruction']}

{test['input']}<|end_of_turn|>GPT4 Correct Assistant:"""
        
        inputs = ft_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = ft_model.generate(
                **inputs,
                pad_token_id=ft_tokenizer.eos_token_id,
                eos_token_id=ft_tokenizer.eos_token_id,
                **gen_config
            )
        
        response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = response.split("GPT4 Correct Assistant:")[-1].strip()
        filtered = filter_obj.filter(generated, max_length=100)
        
        print(f"\ní…ŒìŠ¤íŠ¸ {i}:")
        print(f"ì…ë ¥: {test['input']}")
        print(f"ìƒì„±: {filtered}")
        print(f"ê¸°ëŒ€: {test['expected']}")
        
        results["finetuned"].append({
            "raw": generated,
            "filtered": filtered,
            "valid": filter_obj.validate_korean(filtered)
        })
    
    # ë¹„êµ ë¶„ì„
    print("\n\n" + "=" * 60)
    print("ğŸ“Š ë¹„êµ ë¶„ì„ ê²°ê³¼")
    print("=" * 60)
    
    print("\n### 1. í’ˆì§ˆ ë¹„êµ")
    base_valid = sum(1 for r in results["base"] if r["valid"])
    ft_valid = sum(1 for r in results["finetuned"] if r["valid"])
    
    print(f"ë² ì´ìŠ¤ ëª¨ë¸ ì„±ê³µë¥ : {base_valid}/{len(results['base'])} ({base_valid/len(results['base'])*100:.0f}%)")
    print(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ì„±ê³µë¥ : {ft_valid}/{len(results['finetuned'])} ({ft_valid/len(results['finetuned'])*100:.0f}%)")
    
    print("\n### 2. ì¶œë ¥ íŠ¹ì„± ë¹„êµ")
    print("\në² ì´ìŠ¤ ëª¨ë¸ íŠ¹ì§•:")
    print("- ì¼ë°˜ì ì¸ ëŒ€í™” ëŠ¥ë ¥ ìœ ì§€")
    print("- ë‹¤ì–‘í•œ ì–¸ì–´ í˜¼ì¬ (ì˜ì–´/í•œêµ­ì–´)")
    print("- í”„ë¡¬í”„íŠ¸ ë”°ë¼ê°€ê¸° ëŠ¥ë ¥ ìˆìŒ")
    
    print("\níŒŒì¸íŠœë‹ ëª¨ë¸ íŠ¹ì§•:")
    print("- í•™ìŠµ ë°ì´í„°ì— ê³¼ì í•©")
    print("- ì˜ë¯¸ì—†ëŠ” íŒ¨í„´ ë°˜ë³µ")
    print("- íŠ¹ì • ë„ë©”ì¸ ìš©ì–´ ê³¼ë‹¤ ì‚¬ìš©")
    
    print("\n### 3. ë¬¸ì œì  ë¶„ì„")
    print("\në² ì´ìŠ¤ ëª¨ë¸ ë¬¸ì œ:")
    print("- í•œêµ­ì–´ ë¯¼ì› ë„ë©”ì¸ ì§€ì‹ ë¶€ì¡±")
    print("- ì¼ê´€ì„± ì—†ëŠ” ë‹µë³€ í˜•ì‹")
    
    print("\níŒŒì¸íŠœë‹ ëª¨ë¸ ë¬¸ì œ:")
    print("- 500ìŠ¤í…ìœ¼ë¡œ ë¶€ì¡±í•œ í•™ìŠµ")
    print("- ê³¼ì í•©ìœ¼ë¡œ ì¸í•œ í’ˆì§ˆ ì €í•˜")
    print("- ë°˜ë³µ ë° ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ ìƒì„±")
    
    print("\n### 4. ê°œì„  ë°©í–¥")
    print("\në‹¨ê¸°:")
    print("- ë² ì´ìŠ¤ ëª¨ë¸ + ê°•ë ¥í•œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§")
    print("- í›„ì²˜ë¦¬ í•„í„° í•„ìˆ˜ ì ìš©")
    
    print("\nì¥ê¸°:")
    print("- í•™ìŠµ ë°ì´í„° í’ˆì§ˆ ê°œì„ ")
    print("- ë” ë§ì€ í•™ìŠµ ìŠ¤í… (2000-3000)")
    print("- ë‹¤ë¥¸ ë² ì´ìŠ¤ ëª¨ë¸ ì‹œë„ (SOLAR, Polyglot-Ko)")
    
    # ì‹¤ì œ ì˜ˆì‹œ ë¹„êµ
    print("\n\n### 5. ì‹¤ì œ ì¶œë ¥ ì˜ˆì‹œ")
    print("-" * 60)
    for i in range(len(test_cases)):
        print(f"\ní…ŒìŠ¤íŠ¸ {i+1}:")
        print(f"ë² ì´ìŠ¤: {results['base'][i]['filtered'][:80]}...")
        print(f"íŒŒì¸íŠœë‹: {results['finetuned'][i]['filtered'][:80]}...")
    
    print("\nâœ… ë¶„ì„ ì™„ë£Œ")
    
    return results


def analyze_training_impact():
    """í•™ìŠµì˜ ì˜í–¥ ë¶„ì„"""
    print("\n\n=== íŒŒì¸íŠœë‹ì˜ ì˜í–¥ ë¶„ì„ ===\n")
    
    print("### í•™ìŠµ ë°ì´í„° íŠ¹ì„±")
    print("- ë°ì´í„° í¬ê¸°: 10,259ê°œ í•™ìŠµ ìƒ˜í”Œ")
    print("- í•™ìŠµ ìŠ¤í…: 500 ìŠ¤í…")
    print("- ì†ì‹¤ê°’: 0.71 (ìµœì¢…)")
    print("- í•™ìŠµë¥ : 5e-5 â†’ 6.7e-7 (ê°ì†Œ)")
    
    print("\n### í•™ìŠµ ê³¼ì • ë¬¸ì œ")
    print("1. **ë¶ˆì¶©ë¶„í•œ í•™ìŠµ**")
    print("   - 500ìŠ¤í…ì€ 0.39 ì—í­ì— ë¶ˆê³¼")
    print("   - ëª¨ë¸ì´ íŒ¨í„´ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•¨")
    
    print("\n2. **ê³¼ì í•© ì§•í›„**")
    print("   - íŠ¹ì • íŒ¨í„´ë§Œ ë°˜ë³µ ìƒì„±")
    print("   - í•™ìŠµ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆê¹Œì§€ ì™¸ì›€")
    
    print("\n3. **ë°ì´í„° í’ˆì§ˆ**")
    print("   - ì¤‘ë³µ ë°ì´í„° ì¡´ì¬ ê°€ëŠ¥ì„±")
    print("   - ì¶œë ¥ ê¸¸ì´ ë¶ˆê· í˜•")
    print("   - ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ ê³¼ë‹¤")
    
    print("\n### ë² ì´ìŠ¤ ëª¨ë¸ vs íŒŒì¸íŠœë‹ ëª¨ë¸")
    print("\n| í•­ëª© | ë² ì´ìŠ¤ ëª¨ë¸ | íŒŒì¸íŠœë‹ ëª¨ë¸ |")
    print("|------|------------|--------------|")
    print("| ì¼ë°˜ ëŒ€í™” ëŠ¥ë ¥ | âœ… ìœ ì§€ | âŒ ì†ìƒ |")
    print("| í•œêµ­ì–´ í’ˆì§ˆ | ğŸ”¶ ë³´í†µ | âŒ ë‚®ìŒ |")
    print("| ë°˜ë³µ ë¬¸ì œ | ğŸ”¶ ê°€ë” | âŒ ì‹¬ê° |")
    print("| ë„ë©”ì¸ ì§€ì‹ | âŒ ë¶€ì¡± | ğŸ”¶ ì¼ë¶€ |")
    print("| í”„ë¡¬í”„íŠ¸ ë”°ë¼ê°€ê¸° | âœ… ì–‘í˜¸ | âŒ ë¶ˆëŸ‰ |")
    print("| ì˜ˆì¸¡ ê°€ëŠ¥ì„± | ğŸ”¶ ë³´í†µ | âŒ ë‚®ìŒ |")
    
    print("\n### ê²°ë¡ ")
    print("í˜„ì¬ íŒŒì¸íŠœë‹ ëª¨ë¸ì€ ë² ì´ìŠ¤ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§")
    print("ì›ì¸: ë¶ˆì¶©ë¶„í•œ í•™ìŠµ + ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ")
    print("í•´ê²°: ì¬í•™ìŠµ í•„ìš” (3-5 ì—í­, ë” ë‚˜ì€ ë°ì´í„°)")


if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\n")
        torch.cuda.empty_cache()
    
    # ëª¨ë¸ ë¹„êµ
    results = compare_models()
    
    # í•™ìŠµ ì˜í–¥ ë¶„ì„
    analyze_training_impact()